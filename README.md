In this (ongoing) project, we aim to:

\begin{enumerate}
    \item Measure and establish the presence of reward over-optimization in DPO-RLHF.
    \item Explore current methods of mitigating reward over-optimization (e.g. [scaling up reward models](https://arxiv.org/abs/2210.10760), data augmentation, reward head ensembles) and extend them to DPO, a setting without a static proxy reward.
\end{enumerate}

The relevance of this study lies in its potential to inform the future development of language models which are trained using DPO, ensuring they more closely align to the human preferences.    

[https://drive.google.com/file/d/1M53VUZY2pd6C-fa39RBsO-bbpNxucl_7/view?usp=sharing](poster)

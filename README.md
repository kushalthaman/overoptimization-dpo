In this (ongoing) project, we aim to:
- Measure and establish the presence of reward over-optimization in DPO-RLHF.
- Explore current methods of mitigating reward over-optimization (e.g. [scaling up reward models](https://arxiv.org/abs/2210.10760), [data augmentation](https://arxiv.org/abs/1710.09412), [reward head ensembles](https://openreview.net/forum?id=dcjtMYkpXx)) and extend them to DPO, a setting without a static proxy reward.

The relevance of this study lies in its potential to inform the future development of language models which are trained using DPO, ensuring they more closely align to the human preferences. For more details on some of our initial results, see this [poster](https://drive.google.com/file/d/1M53VUZY2pd6C-fa39RBsO-bbpNxucl_7/view?usp=sharing). 
